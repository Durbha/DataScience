{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def collect_sqlm(num1,num2,queue):\n",
    "    for i in range (num1,num2):\n",
    "        tablenames = []\n",
    "        operand_size = 0\n",
    "        index_size = 0\n",
    "        log_os = 0\n",
    "        log_is = 0\n",
    "        regex =[]\n",
    "        query = initial_sqlm_df.loc[i,('query')].upper()\n",
    "        query = re.sub(r',\\s+', ',', query)\n",
    "        if re.search(r'\\ASELECT|INSERT|DELETE',query):\n",
    "            regex = re.findall(r'(?<=FROM|JOIN|INTO)\\s+[\\w.]+',query)\n",
    "        if re.search(r'\\AUPDATE',query):\n",
    "            regex = re.findall(r'(?<=UPDATE)\\s+[\\w.]+',query)\n",
    "        if re.search(r'\\AMERGE',query):\n",
    "            regex1 = re.findall(r'(?<=INTO)\\s+[\\w.]+',query)\n",
    "            regex2 = re.findall(r'(?<=MERGE)\\s+[\\w.]+',query)\n",
    "            regex = regex1+regex2\n",
    "        for item in regex:\n",
    "            for table in item.split(','):\n",
    "                tablenames.append(table)\n",
    "        for table in tablenames:\n",
    "            tableschema = ''\n",
    "            tablename = ''\n",
    "            if len(table.split('.')) == 2:\n",
    "                tableschema = table.split('.')[0]\n",
    "                tablename = table.split('.')[1]\n",
    "                try:\n",
    "                    operand_size = operand_size + sizedf.loc[tableschema.strip(),tablename.strip()].object_size\n",
    "                    \n",
    "                except KeyError:\n",
    "                    operand_size = 0\n",
    "                try:\n",
    "                    index_size = index_size + sizedf.loc[tableschema.strip(),tablename.strip()].index_size\n",
    "                except KeyError:\n",
    "                    index_size = 0\n",
    "        num_operations = 0\n",
    "        \n",
    "        base_operation = 'SELECT|INSERT|DELETE|UPDATE|MERGE|VALUES'\n",
    "        set_operation = 'JOIN|UNION'\n",
    "        final_operation = 'ORDER BY| GROUP BY'\n",
    "        regex = re.findall(base_operation,query)\n",
    "        num_operations = num_operations+len(regex)\n",
    "        regex = re.findall(set_operation,query)\n",
    "        num_operations = num_operations+len(regex)\n",
    "        regex = re.findall(final_operation,query)\n",
    "        num_operations = num_operations+len(regex)\n",
    "        \n",
    "        num_filters = 0\n",
    "        regex = re.findall(r'WHERE',query)\n",
    "        num_filters = len(regex)\n",
    "        regex = re.findall(r' AND | OR ',query)\n",
    "        num_filters = num_filters + len(regex)\n",
    "        \n",
    "        log_exec_time = m.log2(initial_sqlm_df.loc[i,('exec_time')])\n",
    "        \n",
    "        objects_size_tuple = (i,operand_size/1024/1024,index_size/1024/1024,num_operations,num_filters,log_exec_time)\n",
    "        queue.put(objects_size_tuple)\n",
    "    queue.put((-1,-1,-1,-1,-1,-1))\n",
    "    return\n",
    "\n",
    "def read_queue(queue):\n",
    "    global sizobj_list\n",
    "    global num_procs\n",
    "    count = 0\n",
    "    while True:\n",
    "            location , operand_size,index_size,num_operations,num_filters,log_exec_time = queue.get()\n",
    "            if (operand_size == -1):\n",
    "                count = count + 1\n",
    "                #print (count)\n",
    "            else:\n",
    "                sqlm_list.append((location,operand_size,index_size,num_operations,num_filters,log_exec_time))\n",
    "            if (count == num_procs):\n",
    "                print (\"done\")\n",
    "                break\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = r'/data/developer/python'    \n",
    "allFiles = glob.glob(path + \"/SQL_Metrics*.csv\")     \n",
    "list_ = []\n",
    "notvalues = []\n",
    "for file_ in allFiles:\n",
    "    regex = re.search('SQL_Metrics_([0-9]*-[0-9]*-[0-9]*)-([0-9]*)', file_)\n",
    "    df = pd.read_csv(file_,sep = '@',names = ['stmt_hash','query','exec_time'],index_col=False)\n",
    "    df['date'] = regex.group(1)\n",
    "    df['hour'] = regex.group(2)\n",
    "    df.exec_time = pd.to_numeric(df.exec_time,errors='coerce')\n",
    "    df.hour = pd.to_numeric(df.hour,errors='coerce')\n",
    "    list_.append(df)\n",
    "initial_sqlm_df = pd.concat(list_,ignore_index=True)\n",
    "initial_sqlm_df = initial_sqlm_df[initial_sqlm_df.exec_time > 0].reset_index().drop('index',axis=1)       \n",
    "\n",
    "\n",
    "\n",
    "allFiles = glob.glob(path + \"/size*.csv\")     \n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    regex = re.search('size*', file_)\n",
    "    sizedf = pd.read_csv(file_,sep = '@',names = ['tableschema','tablename','object_size','index_size','lob_size'],index_col=False)\n",
    "    sizedf = sizedf.set_index(['tableschema', 'tablename'])\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "\n",
    "fields = ['time','lock_wait_time','total_section_sort_time','diaglog_write_wait_time','direct_read_time','direct_write_time','log_buffer_wait_time','log_disk_wait_time','pool_read_time','pool_write_time','prefetch_wait_time','total_act_time','total_act_wait_time','total_cpu_time','total_extended_latch_wait_time','span']\n",
    "path = r'/data/developer/python'    \n",
    "allFiles = glob.glob(path + \"/workload_stats_*.csv\")    \n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    regex = re.search('workload_stats_([0-9]*-[0-9]*-[0-9]*)', file_)\n",
    "    wldf = pd.read_csv(file_,sep = '@',names = fields,index_col=False)\n",
    "    list_.append(wldf)\n",
    "final_wldf = pd.concat(list_,ignore_index=True)\n",
    "final_wldf['hour'] = 0\n",
    "final_wldf['date'] = 0\n",
    "for i in range (0,len(final_wldf)):\n",
    "    regex = re.search('([0-9-]*)T([0-9]*)', final_wldf.loc[i,('time')])\n",
    "    final_wldf.loc[i,('date')] = regex.group(1)\n",
    "    final_wldf.loc[i,('hour')] = int(regex.group(2)) + 5\n",
    "    \n",
    "final_wldf = final_wldf.set_index(['date', 'hour'])\n",
    "final_wldf.drop('time',axis=1,inplace = True)\n",
    "final_wldf.drop('span',axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_wldf['log_lock_wait_time'] = 0\n",
    "final_wldf['log_total_section_sort_time'] = 0\n",
    "final_wldf['log_diaglog_write_wait_time'] = 0\n",
    "final_wldf['log_direct_read_time'] = 0\n",
    "final_wldf['log_direct_write_time'] = 0\n",
    "final_wldf['log_log_buffer_wait_time'] = 0\n",
    "final_wldf['log_log_disk_wait_time'] = 0\n",
    "final_wldf['log_pool_read_time'] = 0\n",
    "final_wldf['log_pool_write_time'] = 0\n",
    "final_wldf['log_prefetch_wait_time'] = 0\n",
    "final_wldf['log_total_act_time'] = 0\n",
    "final_wldf['log_total_act_wait_time'] = 0\n",
    "final_wldf['log_total_cpu_time'] = 0\n",
    "final_wldf['log_total_extended_latch_wait_time'] = 0\n",
    "for i in range (0,len(final_wldf)):\n",
    "        if (final_wldf.iloc[i].lock_wait_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_lock_wait_time')] = m.log2(final_wldf.iloc[i].lock_wait_time)\n",
    "        if (final_wldf.iloc[i].total_section_sort_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_total_section_sort_time')] = m.log2(final_wldf.iloc[i].total_section_sort_time)\n",
    "        if (final_wldf.iloc[i].diaglog_write_wait_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_diaglog_write_wait_time')] = m.log2(final_wldf.iloc[i].diaglog_write_wait_time)\n",
    "        if (final_wldf.iloc[i].direct_read_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_direct_read_time')] = m.log2(final_wldf.iloc[i].direct_read_time)\n",
    "        if (final_wldf.iloc[i].direct_write_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_direct_write_time')] = m.log2(final_wldf.iloc[i].direct_write_time)\n",
    "        if (final_wldf.iloc[i].log_buffer_wait_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_log_buffer_wait_time')] = m.log2(final_wldf.iloc[i].log_buffer_wait_time)\n",
    "        if (final_wldf.iloc[i].log_disk_wait_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_log_disk_wait_time')] = m.log2(final_wldf.iloc[i].log_disk_wait_time)\n",
    "        if (final_wldf.iloc[i].pool_read_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_pool_read_time')] = m.log2(final_wldf.iloc[i].pool_read_time)\n",
    "        if (final_wldf.iloc[i].pool_write_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_pool_write_time')] = m.log2(final_wldf.iloc[i].pool_write_time)\n",
    "        if (final_wldf.iloc[i].prefetch_wait_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_prefetch_wait_time')] = m.log2(final_wldf.iloc[i].prefetch_wait_time)\n",
    "        if (final_wldf.iloc[i].total_act_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_total_act_time')] = m.log2(final_wldf.iloc[i].total_act_time)\n",
    "        if (final_wldf.iloc[i].total_act_wait_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_total_act_wait_time')] = m.log2(final_wldf.iloc[i].total_act_wait_time)\n",
    "        if (final_wldf.iloc[i].total_cpu_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_total_cpu_time')] = m.log2(final_wldf.iloc[i].total_cpu_time)\n",
    "        if (final_wldf.iloc[i].total_extended_latch_wait_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_total_extended_latch_wait_time')] = m.log2(final_wldf.iloc[i].total_extended_latch_wait_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Took 166.62014484405518 seconds with 8 processes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "queue = Queue()\n",
    "num_procs = 8\n",
    "sqlm_list = []\n",
    "pro_list = []\n",
    "num_rows = len(initial_sqlm_df)\n",
    "group_size = int(num_rows/num_procs)\n",
    "for group_num in range(0,num_procs):\n",
    "    num1 = group_num*(group_size)\n",
    "    num2 = num1 + group_size - 1\n",
    "    proc = Process(target=collect_sqlm, args=(num1,num2,queue))\n",
    "    pro_list.append(proc)\n",
    "\n",
    "start = time.time()\n",
    "for p in pro_list:\n",
    "    p.start()\n",
    "    \n",
    "read_queue(queue)\n",
    "\n",
    "intermediate_sqlm_df = pd.DataFrame(sqlm_list, columns=['location', 'operand_size','index_size','num_operations','num_filters','log_exec_time']).set_index('location')\n",
    "\n",
    "sqlm_df = initial_sqlm_df.merge(intermediate_sqlm_df, left_index = True, right_index=True, how='inner').reset_index().drop('index',axis = 1)\n",
    "\n",
    "sqlm_df = sqlm_df[sqlm_df.operand_size != 0]\n",
    "#sqlm_df = sqlm_df.set_index(['date', 'hour'])\n",
    "\n",
    "processtime = time.time() - start\n",
    "print(\"Took {} seconds with 8 processes.\".format(processtime))\n",
    "\n",
    "sqlm_grpby_df = sqlm_df.groupby(['date','hour'])['operand_size','index_size','num_operations','num_filters','exec_time','log_exec_time'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df = sqlm_grpby_df.merge(final_wldf, left_index = True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df = final_df.reset_index()\n",
    "final_df = final_df[final_df.index!=6]\n",
    "final_df = final_df[final_df.index!=135]\n",
    "final_df = final_df[final_df.index!=148]\n",
    "final_df = final_df[final_df.index!=147]\n",
    "final_df = final_df[final_df.index!=139]\n",
    "final_df = final_df[final_df.index!=140]\n",
    "final_df = final_df[final_df.index!=138]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021589509338300651"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = final_df [['log_total_cpu_time','num_operations','operand_size']]\n",
    "y = final_df['log_exec_time']\n",
    " \n",
    "est = smf.OLS(y, X)\n",
    "est = est.fit()\n",
    "est.summary()\n",
    "est.mse_resid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
