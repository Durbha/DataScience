{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code is used to collect and process data to compute the features, which is then used to train and estimate a regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two functions defined below are called by multiple processes in parallel\n",
    "1.**collect_sqlm** - This function takes three inputs, the start and end point (which give the number of queries to be processed) and the dataframe with the queries. Passing the start and end point allows this function to be called in parallel by different processes.In this case, we use 8 processes to cut down the processing time by more than half. After processing the data we write the computed values (operand size,index size,number of perations,number of filters,log of execution time) to a queue. \n",
    "    \n",
    "2.**read_queue** - As the processes write data to the queue, another process starts to consume the values from the queue to write it to a data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def collect_sqlm(num1,num2,queue):\n",
    "    \n",
    "    #For each query in the given range allocated to the process calling this function.\n",
    "    for i in range (num1,num2):\n",
    "        \n",
    "        #Declare the variable names to store the metrics to be computed.\n",
    "        tablenames = []\n",
    "        operand_size = 0\n",
    "        index_size = 0\n",
    "        num_operations = 0\n",
    "        num_filters = 0\n",
    "        log_os = 0\n",
    "        log_is = 0\n",
    "        regex = []\n",
    "        \n",
    "        #The following regular expressions parse the query to identify the table names\n",
    "        query = initial_sqlm_df.loc[i,('query')].upper()\n",
    "        query = re.sub(r',\\s+', ',', query)\n",
    "        if re.search(r'\\ASELECT|INSERT|DELETE',query):\n",
    "            regex = re.findall(r'(?<=FROM|JOIN|INTO)\\s+[\\w.]+',query)\n",
    "        if re.search(r'\\AUPDATE',query):\n",
    "            regex = re.findall(r'(?<=UPDATE)\\s+[\\w.]+',query)\n",
    "        if re.search(r'\\AMERGE',query):\n",
    "            regex1 = re.findall(r'(?<=INTO)\\s+[\\w.]+',query)\n",
    "            regex2 = re.findall(r'(?<=MERGE)\\s+[\\w.]+',query)\n",
    "            regex = regex1+regex2\n",
    "        for item in regex:\n",
    "            for table in item.split(','):\n",
    "                tablenames.append(table)\n",
    "        \n",
    "        #The following loop computes the operand size and the index size\n",
    "        for table in tablenames:\n",
    "            tableschema = ''\n",
    "            tablename = ''\n",
    "            if len(table.split('.')) == 2:\n",
    "                tableschema = table.split('.')[0]\n",
    "                tablename = table.split('.')[1]\n",
    "                try:\n",
    "                    operand_size = operand_size + sizedf.loc[tableschema.strip(),tablename.strip()].object_size\n",
    "                    \n",
    "                except KeyError:\n",
    "                    operand_size = 0\n",
    "                try:\n",
    "                    index_size = index_size + sizedf.loc[tableschema.strip(),tablename.strip()].index_size\n",
    "                except KeyError:\n",
    "                    index_size = 0\n",
    "                    \n",
    "        #This section computes the number of operations in the query\n",
    "        base_operation = 'SELECT|INSERT|DELETE|UPDATE|MERGE|VALUES'\n",
    "        set_operation = 'JOIN|UNION'\n",
    "        final_operation = 'ORDER BY| GROUP BY'\n",
    "        regex = re.findall(base_operation,query)\n",
    "        num_operations = num_operations+len(regex)\n",
    "        regex = re.findall(set_operation,query)\n",
    "        num_operations = num_operations+len(regex)\n",
    "        regex = re.findall(final_operation,query)\n",
    "        num_operations = num_operations+len(regex)\n",
    "        \n",
    "        #This section computes the number of filters in the query\n",
    "        regex = re.findall(r'WHERE',query)\n",
    "        num_filters = len(regex)\n",
    "        regex = re.findall(r' AND | OR ',query)\n",
    "        num_filters = num_filters + len(regex)\n",
    "        \n",
    "        objects_size_tuple = (i,operand_size/1024/1024,index_size/1024/1024,num_operations,num_filters)\n",
    "        queue.put(objects_size_tuple)\n",
    "        \n",
    "    #The process wites a series of '-1' to indicate that the process has completed its set of queries.\n",
    "    queue.put((-1,-1,-1,-1,-1))\n",
    "    return\n",
    "\n",
    "def read_queue(queue):\n",
    "    global sizobj_list\n",
    "    global num_procs\n",
    "    count = 0\n",
    "    while True:\n",
    "            location , operand_size,index_size,num_operations,num_filters = queue.get()\n",
    "            if (operand_size == -1):\n",
    "                count = count + 1\n",
    "                #print (count)\n",
    "            else:\n",
    "                sqlm_list.append((location,operand_size,index_size,num_operations,num_filters))\n",
    "            if (count == num_procs):\n",
    "                print (\"done\")\n",
    "                break\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code creates two data frames:\n",
    "    1. SQL data \n",
    "    2. Workload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SQL data. The data is collected in multiple files. Each file corresponding to the SQL cache in a given one hour \n",
    "#window\n",
    "path = r'/data/developer/python'    \n",
    "allFiles = glob.glob(path + \"/SQL_Metrics*.csv\")     \n",
    "list_ = []\n",
    "notvalues = []\n",
    "for file_ in allFiles:\n",
    "    regex = re.search('SQL_Metrics_([0-9]*-[0-9]*-[0-9]*)-([0-9]*)', file_)\n",
    "    df = pd.read_csv(file_,sep = '@',names = ['stmt_hash','query','exec_time'],index_col=False)\n",
    "    df['date'] = regex.group(1)\n",
    "    df['hour'] = regex.group(2)\n",
    "    df.exec_time = pd.to_numeric(df.exec_time,errors='coerce')\n",
    "    df.hour = pd.to_numeric(df.hour,errors='coerce')\n",
    "    list_.append(df)\n",
    "initial_sqlm_df = pd.concat(list_,ignore_index=True)\n",
    "initial_sqlm_df = initial_sqlm_df[initial_sqlm_df.exec_time > 0].reset_index().drop('index',axis=1)       \n",
    "\n",
    "#Table/index sizes in the database needed to compute the table and indexes being in a query\n",
    "allFiles = glob.glob(path + \"/size*.csv\")     \n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    regex = re.search('size*', file_)\n",
    "    sizedf = pd.read_csv(file_,sep = '@',names = ['tableschema','tablename',\n",
    "                                                  'object_size','index_size',\n",
    "                                                  'lob_size'],index_col=False)\n",
    "    sizedf = sizedf.set_index(['tableschema', 'tablename'])\n",
    "\n",
    "\n",
    "#Workload Metrics\n",
    "fields = ['time','lock_wait_time','total_section_sort_time','diaglog_write_wait_time',\n",
    "          'direct_read_time','direct_write_time','log_buffer_wait_time','log_disk_wait_time',\n",
    "          'pool_read_time','pool_write_time','prefetch_wait_time','total_act_time','total_act_wait_time',\n",
    "          'total_cpu_time','total_extended_latch_wait_time','span']\n",
    "\n",
    "path = r'/data/developer/python'    \n",
    "allFiles = glob.glob(path + \"/workload_stats_*.csv\")    \n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    regex = re.search('workload_stats_([0-9]*-[0-9]*-[0-9]*)', file_)\n",
    "    wldf = pd.read_csv(file_,sep = '@',names = fields,index_col=False)\n",
    "    list_.append(wldf)\n",
    "final_wldf = pd.concat(list_,ignore_index=True)\n",
    "final_wldf['hour'] = 0\n",
    "final_wldf['date'] = 0\n",
    "for i in range (0,len(final_wldf)):\n",
    "    regex = re.search('([0-9-]*)T([0-9]*)', final_wldf.loc[i,('time')])\n",
    "    final_wldf.loc[i,('date')] = regex.group(1)\n",
    "    final_wldf.loc[i,('hour')] = int(regex.group(2)) + 5\n",
    "    \n",
    "final_wldf = final_wldf.set_index(['date', 'hour'])\n",
    "final_wldf.drop('time',axis=1,inplace = True)\n",
    "final_wldf.drop('span',axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We compute the log transform of all the workload metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_wldf['log_lock_wait_time'] = 0\n",
    "final_wldf['log_total_section_sort_time'] = 0\n",
    "final_wldf['log_diaglog_write_wait_time'] = 0\n",
    "final_wldf['log_direct_read_time'] = 0\n",
    "final_wldf['log_direct_write_time'] = 0\n",
    "final_wldf['log_log_buffer_wait_time'] = 0\n",
    "final_wldf['log_log_disk_wait_time'] = 0\n",
    "final_wldf['log_pool_read_time'] = 0\n",
    "final_wldf['log_pool_write_time'] = 0\n",
    "final_wldf['log_prefetch_wait_time'] = 0\n",
    "final_wldf['log_total_act_time'] = 0\n",
    "final_wldf['log_total_act_wait_time'] = 0\n",
    "final_wldf['log_total_cpu_time'] = 0\n",
    "final_wldf['log_total_extended_latch_wait_time'] = 0\n",
    "for i in range (0,len(final_wldf)):\n",
    "        if (final_wldf.iloc[i].lock_wait_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_lock_wait_time')\n",
    "                          ] = m.log2(final_wldf.iloc[i].lock_wait_time)\n",
    "        if (final_wldf.iloc[i].total_section_sort_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_total_section_sort_time')\n",
    "                          ] = m.log2(final_wldf.iloc[i].total_section_sort_time)\n",
    "        if (final_wldf.iloc[i].diaglog_write_wait_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_diaglog_write_wait_time')\n",
    "                          ] = m.log2(final_wldf.iloc[i].diaglog_write_wait_time)\n",
    "        if (final_wldf.iloc[i].direct_read_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_direct_read_time')\n",
    "                          ] = m.log2(final_wldf.iloc[i].direct_read_time)\n",
    "        if (final_wldf.iloc[i].direct_write_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_direct_write_time')\n",
    "                          ] = m.log2(final_wldf.iloc[i].direct_write_time)\n",
    "        if (final_wldf.iloc[i].log_buffer_wait_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_log_buffer_wait_time')\n",
    "                          ] = m.log2(final_wldf.iloc[i].log_buffer_wait_time)\n",
    "        if (final_wldf.iloc[i].log_disk_wait_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_log_disk_wait_time')\n",
    "                          ] = m.log2(final_wldf.iloc[i].log_disk_wait_time)\n",
    "        if (final_wldf.iloc[i].pool_read_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_pool_read_time')\n",
    "                          ] = m.log2(final_wldf.iloc[i].pool_read_time)\n",
    "        if (final_wldf.iloc[i].pool_write_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_pool_write_time')\n",
    "                          ] = m.log2(final_wldf.iloc[i].pool_write_time)\n",
    "        if (final_wldf.iloc[i].prefetch_wait_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_prefetch_wait_time')\n",
    "                          ] = m.log2(final_wldf.iloc[i].prefetch_wait_time)\n",
    "        if (final_wldf.iloc[i].total_act_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_total_act_time')\n",
    "                          ] = m.log2(final_wldf.iloc[i].total_act_time)\n",
    "        if (final_wldf.iloc[i].total_act_wait_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_total_act_wait_time')\n",
    "                          ] = m.log2(final_wldf.iloc[i].total_act_wait_time)\n",
    "        if (final_wldf.iloc[i].total_cpu_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_total_cpu_time')\n",
    "                          ] = m.log2(final_wldf.iloc[i].total_cpu_time)\n",
    "        if (final_wldf.iloc[i].total_extended_latch_wait_time!=0):\n",
    "           final_wldf.iloc[i,final_wldf.columns.get_loc('log_total_extended_latch_wait_time')\n",
    "                          ] = m.log2(final_wldf.iloc[i].total_extended_latch_wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We start 8 parallel processes and invoke collect_sqlm to compute the SQL metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "queue = Queue()\n",
    "num_procs = 8\n",
    "sqlm_list = []\n",
    "pro_list = []\n",
    "num_rows = len(initial_sqlm_df)\n",
    "group_size = int(num_rows/num_procs)\n",
    "for group_num in range(0,num_procs):\n",
    "    num1 = group_num*(group_size)\n",
    "    num2 = num1 + group_size - 1\n",
    "    proc = Process(target=collect_sqlm, args=(num1,num2,queue))\n",
    "    pro_list.append(proc)\n",
    "\n",
    "start = time.time()\n",
    "for p in pro_list:\n",
    "    p.start()\n",
    "    \n",
    "read_queue(queue)\n",
    "\n",
    "intermediate_sqlm_df = pd.DataFrame(sqlm_list, columns=['location', 'operand_size','index_size',\n",
    "                                    'num_operations','num_filters']).set_index('location')\n",
    "\n",
    " \n",
    "sqlm_df = initial_sqlm_df.merge(intermediate_sqlm_df, left_index = True, \n",
    "                                right_index=True, how='inner').reset_index().drop('index',axis = 1)\n",
    "\n",
    "sqlm_df = sqlm_df[sqlm_df.operand_size != 0]\n",
    "#sqlm_df = sqlm_df.set_index(['date', 'hour'])\n",
    "\n",
    "processtime = time.time() - start\n",
    "print(\"Took {} seconds with 8 processes.\".format(processtime))\n",
    "\n",
    "sqlm_grpby_df = sqlm_df.groupby(['date','hour'])['operand_size','index_size','num_operations',\n",
    "                                                 'num_filters','exec_time'].mean()\n",
    "\n",
    "final_df = final_wldf.merge(sqlm_grpby_df, left_index = True, right_index=True, how='inner')\n",
    "\n",
    "print(final_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the workload and SQL metrics into one data frame and remove the outliers based on EDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df = final_df.reset_index()\n",
    "final_df = final_df[final_df.index!=6]\n",
    "final_df = final_df[final_df.index!=135]\n",
    "final_df = final_df[final_df.index!=148]\n",
    "final_df = final_df[final_df.index!=147]\n",
    "final_df = final_df[final_df.index!=139]\n",
    "final_df = final_df[final_df.index!=140]\n",
    "final_df = final_df[final_df.index!=138]\n",
    "final_df['log_exec_time'] = 0\n",
    "#final_df['log_lock_wait_time'] = 0\n",
    "for i in range (0,len(final_df)):\n",
    "        final_df.iloc[i,final_df.columns.get_loc('log_exec_time')] = m.log2(final_df.iloc[i].exec_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#X = final_df[['total_cpu_time','operand_size','num_filters']]\n",
    "#X = final_df[['operand_size','num_filters','log_total_cpu_time']]\n",
    "#X = final_df [['log_total_cpu_time','lock_wait_time','operand_size','num_filters','num_operations']]\n",
    "X = final_df [['log_total_cpu_time','operand_size','num_filters','num_operations']]\n",
    "y = final_df['log_exec_time']\n",
    "\n",
    "#Total CPU Time , Lock time out, operand size , number of filters , number of operations \n",
    "\n",
    "est = smf.OLS(y, X)\n",
    "\n",
    "est = est.fit()\n",
    "est.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
